{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV_8YTRvfpLr"
      },
      "source": [
        "# **RAG Application using DocRetriever**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_LvoZV0f1-x"
      },
      "source": [
        "# **Requirement Phase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJs1XGeRfPIN",
        "outputId": "63f9cb58-17a7-4f16-e026-f5691874387b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.3.4)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.39.0)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (5.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.24.7)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.3.3)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 1)) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 1)) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 1)) (0.3.13)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 1)) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 1)) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 1)) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 1)) (9.0.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit->-r requirements.txt (line 2)) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (16.1.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (13.9.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (6.3.3)\n",
            "Requirement already satisfied: watchdog<6,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 2)) (5.0.3)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client->-r requirements.txt (line 3)) (2024.8.30)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client->-r requirements.txt (line 3)) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client->-r requirements.txt (line 3)) (4.66.5)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client->-r requirements.txt (line 3)) (2.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r requirements.txt (line 4)) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r requirements.txt (line 4)) (2024.6.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community->-r requirements.txt (line 5)) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community->-r requirements.txt (line 5)) (2.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r requirements.txt (line 6)) (4.44.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r requirements.txt (line 6)) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r requirements.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers->-r requirements.txt (line 6)) (1.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 2)) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 2)) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 2)) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 5)) (3.23.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 5)) (0.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 2)) (4.0.11)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain->-r requirements.txt (line 1)) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 1)) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 1)) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit->-r requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit->-r requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 1)) (2.23.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->-r requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->-r requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->-r requirements.txt (line 2)) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 6)) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers->-r requirements.txt (line 6)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 6)) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 6)) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers->-r requirements.txt (line 6)) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers->-r requirements.txt (line 6)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers->-r requirements.txt (line 6)) (3.5.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 2)) (5.0.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 1)) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 1)) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit->-r requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 2)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 2)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 2)) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r requirements.txt (line 2)) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 1)) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r \"requirements.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnDc7bFIgN1o"
      },
      "source": [
        "<hr>\n",
        "\n",
        "# **Data Loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHd9vSDsgS-g"
      },
      "source": [
        "### **Libraries Required**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKh7F2SRgCyO"
      },
      "outputs": [],
      "source": [
        "# This is for input / output operation\n",
        "import os\n",
        "import time\n",
        "# Warning to be ignored\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from google.colab import userdata\n",
        "# This library is for loading textual data\n",
        "from langchain.document_loaders import TextLoader\n",
        "# This library will handle the splitting part of the data\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "# This library will handle embedding of data\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjTDvZMmhKfF"
      },
      "source": [
        "### **Data Loading**\n",
        "  **`HuggingFaceEmbedding` is a class provided by the LangChain library that allows you to use Hugging Face's pre-trained sentence transformer models for text embedding. These embeddings are numerical representations of text, capturing semantic and syntactic information.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfmeYSpxgoG_"
      },
      "outputs": [],
      "source": [
        "# We will create a Text Loader that will help us in loading the document into the environment\n",
        "# TextLoader will expect the path of your data\n",
        "loader = TextLoader(\"/content/Machine Learning Operations.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGYHus6Qh9Mq"
      },
      "source": [
        "#### **Creating the documents for storing the data into DB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Kaf1xQjh5Em"
      },
      "outputs": [],
      "source": [
        "# This process has been, since the data that has to be stored in db has to be in form of document,\n",
        "# MOreover, it's easy to split documents\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceROTXWMjJlP"
      },
      "source": [
        "### **Splitting of data**\n",
        "  **We need to split the data, since the model context window won't be able to get complete information in one go, due to token limit. Hence, we need to break our documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUIF0Vudh7Za"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n19uogC1jGJY",
        "outputId": "83dc0250-2898-476d-a695-cd664aeb41c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1589, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2246, which is longer than the specified 1000\n"
          ]
        }
      ],
      "source": [
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhlUuB0xkNlc"
      },
      "source": [
        "### **Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5daOzsPkKdw",
        "outputId": "def08a0c-8c50-4867-bb9a-4546a1e5a00d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-d0c9174021d8>:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings()\n",
            "<ipython-input-7-d0c9174021d8>:1: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embeddings = HuggingFaceEmbeddings()\n"
          ]
        }
      ],
      "source": [
        "embeddings = HuggingFaceEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ef8Hbhyl9FH"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### **Database Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjgP4SGElX4J"
      },
      "source": [
        "<hr>\n",
        "\n",
        "**Following this embedding part, the next step is to deposit this embedded data into the vector database.**\n",
        "  * **First, we will initialize the pinecone using Pinecone API**\n",
        "  * **We will see, if there is already index there if not we will create one**\n",
        "  * **If it does exist, we link that index to document search variable. If not then we will create one index for us**\n",
        "    * **HuggingFaceEmbedding: dimension: 768, metric = cosine**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzEWUXrsmfbE"
      },
      "source": [
        "<hr>\n",
        "\n",
        "**Index**\n",
        "  * **Your model processes data (like text or images) and converts it into numerical representations called vectors. These vectors capture the semantic meaning and relationships between different pieces of data.**\n",
        "\n",
        "  * **When you want to find similar items, you query the index with a query vector. The index uses the specified metric (in this case, cosine similarity) to find the vectors that are most similar to the query vector.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f9rM6dMnNqa"
      },
      "source": [
        "<hr>\n",
        "\n",
        "#### **PineCone Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3nEcueXkSgI",
        "outputId": "6e754e91-4bdc-42d0-f6d4-e152e2641425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.10/dist-packages (5.3.1)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2024.8.30)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1aMz6dknUUo"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVRc4AEcnjS5"
      },
      "source": [
        "#### **Initialize the Pinecone token**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PquomFGRnidG"
      },
      "outputs": [],
      "source": [
        "pc = Pinecone(api_key = userdata.get(\"PCToken\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_MFuM0dnwzI"
      },
      "source": [
        "#### **Initializing Pinecone Enviroments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0FKF1AVntg4"
      },
      "outputs": [],
      "source": [
        "# We are initializing the cloud platform over here\n",
        "cloud = os.environ.get(\"PINECONE_CLOUD\") or \"aws\"\n",
        "# We are going to give a region for aws\n",
        "region = os.environ.get(\"PINECONE_REGION\") or \"us-east-1\"\n",
        "# Initialize the client\n",
        "serv = ServerlessSpec(cloud = cloud, region = region)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QcVu68uou-_"
      },
      "source": [
        "#### **Create the storage index**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbmEOajFq9KW"
      },
      "source": [
        "**Note: the index should always have lowercase letters with symbols**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdcvjB9CosPP"
      },
      "outputs": [],
      "source": [
        "index_name = \"kart-15th-03\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0lNM6r0o05L",
        "outputId": "5fe8d9a5-3a89-4f1e-ce2e-e7d8ac8b5f4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index before inserting\n",
            "{'dimension': 768,\n",
            " 'index_fullness': 0.0,\n",
            " 'namespaces': {'': {'vector_count': 15}},\n",
            " 'total_vector_count': 15}\n"
          ]
        }
      ],
      "source": [
        "# We are check if the name of our index is not existing in pinecone directory\n",
        "if index_name not in pc.list_indexes().names():\n",
        "  # if not then we will create a index for us\n",
        "  pc.create_index(\n",
        "      name = index_name,\n",
        "      dimension = 768,\n",
        "      metric = \"cosine\",\n",
        "      spec = serv\n",
        "  )\n",
        "  # Waiting till the machine has not created the index\n",
        "  while not pc.describe_index(index_name).status['ready']:\n",
        "    time.sleep(1)\n",
        "\n",
        "# Check to see if the index is ready\n",
        "print(\"Index before inserting\")\n",
        "print(pc.Index(index_name).describe_index_stats())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkjIAFrWqHES"
      },
      "source": [
        "**Cosine Similarity: It's basically comparison between two vector, wherein the angle between two given vectors should be as minimum as possible in order to considered them similar**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohZtgj8SryBk"
      },
      "source": [
        "#### **Adding the data into database**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0g7A_0GqGjx"
      },
      "outputs": [],
      "source": [
        "# INitializing Pinecone token\n",
        "PINECONE_API_KEY = userdata.get(\"PCToken\")\n",
        "# Setup Pinecone into environment\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpX2u_tFsM01"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Pinecone as PineconeVectorStore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoJG36Tpserx"
      },
      "source": [
        "**If the index is not available in the pinecone storage, then we will try to fetch the data directly from the documentation, or else if the index is present then we will try to fetch it from the index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59VUnqKxsTAW"
      },
      "outputs": [],
      "source": [
        "# IF the index is not there in the index list\n",
        "if index_name not in pc.list_indexes():\n",
        "  docsearch = PineconeVectorStore.from_documents(docs, embeddings, index_name = index_name)\n",
        "else:\n",
        "  docsearch = PineconeVectorStore.from_existing_index(index_name, embeddings, pinecone_index = pc.Index(index_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdPD91KbtIvP"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### **Model Setup**\n",
        "\n",
        "  **Since, we ahve the embedded text on vecDB, so we can now create our model, and HuggingFaceHub, we can connect to the model and we can directly use it**\n",
        "  * **We will define the modelID, and in this case `mistralai/Mixtral-8x7B-Instruct-v0.1`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvn60MHjtAS8"
      },
      "outputs": [],
      "source": [
        "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuKYeOL7y3e9"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFaceHub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw2CHrrky81k"
      },
      "source": [
        "#### **LLM Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8cXCNSGy6Vv",
        "outputId": "2f3e192d-a823-4430-ccb9-507c2ee4128c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-3af5131dec17>:1: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  llm = HuggingFaceHub(\n"
          ]
        }
      ],
      "source": [
        "llm = HuggingFaceHub(\n",
        "    repo_id = model_id,\n",
        "    model_kwargs = {\"temperature\" : 0.8, \"top_k\" : 50},\n",
        "    huggingfacehub_api_token = userdata.get(\"HFToken\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i4u2eTZzzeC"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### **Prompt Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu6FGCzHzShy"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lU0O9bc0EjC"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "You are a MLOPs engineer. The user will ask you a question about Machine Learning Operations.\n",
        "Use the following piece of context to answer the question.\n",
        "If you don't know the answer, just say don't know/\n",
        "Keep the answer brief\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer:\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoQwyQJa0oIW"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template = template,\n",
        "    input_variables = [\"context\", \"question\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOgkOa-K0wdT",
        "outputId": "a0c493e1-9fb3-4642-a0f1-3c4f63555ca8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"\\nYou are a MLOPs engineer. The user will ask you a question about Machine Learning Operations.\\nUse the following piece of context to answer the question.\\nIf you don't know the answer, just say don't know/\\nKeep the answer brief\\n\\nContext: {context}\\nQuestion: {question}\\nAnswer:\\n\\n\")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jZEWUZt04bx"
      },
      "source": [
        "### **Chaining it all together**\n",
        "* **What all we are having**\n",
        "  * **Pinecone Database (Embedded Data)**\n",
        "  * **Model (mixtral model)**\n",
        "  * **PromptTemplate**\n",
        "\n",
        "**So, the process begins with the document search wherein we will try to search for the relevant information based on context. Then once the query goes through this, our prompt will start working and then it will return us the output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtBjE_z20xpD"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSTmqC8O15Hf"
      },
      "outputs": [],
      "source": [
        "rag_chain = (\n",
        "    {\"context\" : docsearch.as_retriever(), \"question\" : RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYzXm2uG4Ytt",
        "outputId": "2229ed89-fee4-4c1a-8d8d-fbda71e4f68e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "Till now, we have created all the MLOPs model and trained a lot of models, tested them and done all the aspects related to machine learning aspects. But what's the use to it? Where it is being utilized.\n",
            "Here is where the MLOPs comes into play.:\n",
            "Creating an ML model that can predict what you want it to predict from the data you have fed is easy, but creating a model that is reliable, fast, accurate, pinpoint and can be used by many users in difficult, isn't it?\n",
            "So, that's where the MLOPs comes into the play:\n",
            "•\t These models that rely on large amount of data, are very difficult for a single person to be handled and tracking their development or usage.\n",
            "•\tSince, due to having a lot of data, even if there is small tweak in the parameters it can result in the enormous difference in the results and accuracy.\n",
            "•\tNow, feature engineering is another hectic task that would come up with the large dataset, because we need to keep the track of the features with which the model is working.\n",
            "•\tMonitoring the model isn't easy like monitoring a software performance.\n",
            "•\tDebugging the ML model is extremely painful.\n",
            "•\tNow, here comes the major problem. Since, we guys are working with the real-world data for predictions and all other aspects. So as the real-world data keeps on updating the model should also keep updating itself. This means we need to keep the track of the new data change and accordingly we need to make sure that model learn them.\n",
            "If we guys, take a funny example, as developer we always give excuse, it's working on my end....\n",
            "This is not what we have to do, here in MLOps.\n",
            "\n",
            "Till now, we have created all the MLOPs model and trained a lot of models, tested them and done all the aspects related to machine learning aspects. But what's the use to it? Where it is being utilized.\n",
            "Here is where the MLOPs comes into play.:\n",
            "Creating an ML model that can predict what you want it to predict from the data you have fed is easy, but creating a model that is reliable, fast, accurate, pinpoint and can be used by many users in difficult, isn't it?\n",
            "So, that's where the MLOPs comes into the play:\n",
            "•\t These models that rely on large amount of data, are very difficult for a single person to be handled and tracking their development or usage.\n",
            "•\tSince, due to having a lot of data, even if there is small tweak in the parameters it can result in the enormous difference in the results and accuracy.\n",
            "•\tNow, feature engineering is another hectic task that would come up with the large dataset, because we need to keep the track of the features with which the model is working.\n",
            "•\tMonitoring the model isn't easy like monitoring a software performance.\n",
            "•\tDebugging the ML model is extremely painful.\n",
            "•\tNow, here comes the major problem. Since, we guys are working with the real-world data for predictions and all other aspects. So as the real-world data keeps on updating the model should also keep updating itself. This means we need to keep the track of the new data change and accordingly we need to make sure that model learn them.\n",
            "If we guys, take a funny example, as developer we always give excuse, it's working on my end....\n",
            "This is not what we have to do, here in MLOps.\n",
            "\n",
            "Machine Learning Operations\n",
            "\n",
            "The basic introduction combines of what exactly MLOPS and its auxiliaries. Moreover, what exactly it is.\n",
            "Definition:\n",
            "Machine learning operations (MLOps) is the development and use of machine learning models by development operations (DevOps) teams. \n",
            "Machine Learning Operations involves a set of processes or rather a sequence of steps implemented to deploy an ML model to the production environment. There are several steps to be undertaken before an ML Model is production ready. These processes ensure that your model can be scaled for a large user base and perform accurately.\n",
            "\n",
            "What is the use of the MLOps?\n",
            "\n",
            "Machine Learning Operations\n",
            "\n",
            "The basic introduction combines of what exactly MLOPS and its auxiliaries. Moreover, what exactly it is.\n",
            "Definition:\n",
            "Machine learning operations (MLOps) is the development and use of machine learning models by development operations (DevOps) teams. \n",
            "Machine Learning Operations involves a set of processes or rather a sequence of steps implemented to deploy an ML model to the production environment. There are several steps to be undertaken before an ML Model is production ready. These processes ensure that your model can be scaled for a large user base and perform accurately.\n",
            "\n",
            "What is the use of the MLOps?\n",
            "\n",
            "Question: What is MLOps?\n",
            "Helpful Answer: MLOps is the development and use of machine learning models by DevOps teams. It is a set of processes or a sequence of steps implemented to deploy an ML model to the production environment, ensuring that it can be scaled for a large user base and perform accurately.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from pinecone import Pinecone\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=model_id,\n",
        "    model_kwargs={\"temperature\": 0.8, \"top_k\": 50},\n",
        "    huggingfacehub_api_token=userdata.get(\"HFToken\")\n",
        ")\n",
        "\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=docsearch.as_retriever(),\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "query = \"What is MLOps?\"\n",
        "result = qa_chain.run(query)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmse62rm2eWj"
      },
      "source": [
        "<hr>\n",
        "\n",
        "### **Finalize the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM2w4e3d2c8H"
      },
      "outputs": [],
      "source": [
        "class Chatbot():\n",
        "  loader = TextLoader(\"/content/Machine Learning Operations.txt\")\n",
        "  documents = loader.load()\n",
        "\n",
        "  rag_chain = (\n",
        "    {\"context\" : docsearch.as_retriever(), \"question\" : RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJaUw96X2uGv"
      },
      "outputs": [],
      "source": [
        "bot = Chatbot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLMmz-BE3A_e",
        "outputId": "bc53baee-3617-43c1-b2a6-9c9281bb4e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask AnythingWhat is CI/CD\n",
            "\n",
            "You are a MLOPs engineer. The user will ask you a question about Machine Learning Operations.\n",
            "Use the following piece of context to answer the question.\n",
            "If you don't know the answer, just say don't know/\n",
            "Keep the answer brief\n",
            "\n",
            "Context: [Document(metadata={'source': '/content/Machine Learning Operations.txt'}, page_content='This will not be much beneficial for some of the particular \\nas some models might require learning from the user inputs and predictions it makes. This lifecycle is valid for most of the ML use cases.\\n\\nUNDERSTANDING THE CI/CD PIPELINE\\n\\nIn development, whenever we update the code, we want that the code should be updated everywhere it is being used, ensuring that each user is having the same functionality of it, on their respective devices. Now this seems as easy as it could be but is as complicated as it could be.\\n\\nCI/CD ensures that the integration and delivery of incremental changes to a live application. It is triggered when by a new update of version control system. This integration helps the system to go through all the stages until they safely reach the production environment.'), Document(metadata={'source': '/content/Machine Learning Operations.txt'}, page_content='This will not be much beneficial for some of the particular \\nas some models might require learning from the user inputs and predictions it makes. This lifecycle is valid for most of the ML use cases.\\n\\nUNDERSTANDING THE CI/CD PIPELINE\\n\\nIn development, whenever we update the code, we want that the code should be updated everywhere it is being used, ensuring that each user is having the same functionality of it, on their respective devices. Now this seems as easy as it could be but is as complicated as it could be.\\n\\nCI/CD ensures that the integration and delivery of incremental changes to a live application. It is triggered when by a new update of version control system. This integration helps the system to go through all the stages until they safely reach the production environment.'), Document(metadata={'source': '/content/Machine Learning Operations.txt'}, page_content=\"Reproducible results: You can be confident that any changes in model behaviour are due to actual data or code changes, not accidental differences in setups.\\nEasier troubleshooting: It's simpler to pinpoint the source of issues when everyone is using the same tools and steps.\\nImproved collaboration: Different data scientists can easily share and understand each other's work.\\nThink of it like building a Lego set. Each person gets the same instructions and pieces, resulting in the same finished product every time. This makes teamwork and consistency much easier.\\n\\nContinuous Integration / Continuous Delivery (CI/CD), originating from and gaining prominence in Software Development, is centred around the idea of regularly delivering incremental changes to a live application via an automated process of rebuilding, retesting, and redeploying.\"), Document(metadata={'source': '/content/Machine Learning Operations.txt'}, page_content=\"Reproducible results: You can be confident that any changes in model behaviour are due to actual data or code changes, not accidental differences in setups.\\nEasier troubleshooting: It's simpler to pinpoint the source of issues when everyone is using the same tools and steps.\\nImproved collaboration: Different data scientists can easily share and understand each other's work.\\nThink of it like building a Lego set. Each person gets the same instructions and pieces, resulting in the same finished product every time. This makes teamwork and consistency much easier.\\n\\nContinuous Integration / Continuous Delivery (CI/CD), originating from and gaining prominence in Software Development, is centred around the idea of regularly delivering incremental changes to a live application via an automated process of rebuilding, retesting, and redeploying.\")]\n",
            "Question: What is CI/CD\n",
            "Answer:\n",
            "\n",
            "CI/CD stands for Continuous Integration / Continuous Delivery. It is a practice in software development that focuses on regularly delivering incremental changes to a live application through an automated process of rebuilding, retesting, and redeploying. This ensures reproducible results, easier troubleshooting, and improved collaboration among team members.\n"
          ]
        }
      ],
      "source": [
        "a = input(\"Ask Anything\")\n",
        "res = bot.rag_chain.invoke(a)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK_YUICb3IkZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
